"""
Chatbot de Recherche Acad√©mique - Assistant IA pour M√©moire de Fin d'√âtudes
==============================================================================
Projet : Approches intelligentes et math√©matiques pour analyser le churn client
Auteur : Master Mod√©lisation Math√©matique et Science de Donn√©es
Technologies : LangChain, FAISS, Hugging Face, Streamlit
"""

import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain_community.vectorstores import FAISS
#from langchain.chains import RetrievalQA
#from langchain_community.chains import RetrievalQA
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.prompts import PromptTemplate

# Chargement des variables d'environnement
load_dotenv()

# Configuration de la page Streamlit
st.set_page_config(
    page_title="üéì Assistant IA - M√©moire de Recherche",
    page_icon="üìö",
    layout="wide"
)

# ==================== CONFIGURATION ====================

# Chemins des fichiers
PDF_PATH = "data/memoire.pdf"
VECTOR_STORE_PATH = "vector_store/faiss_index"

# Token Hugging Face (depuis .env)
HF_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN")

# Mod√®le LLM et Embeddings
#LLM_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
LLM_MODEL = "meta-llama/Llama-3.1-8B"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# Param√®tres de d√©coupage du texte
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200


# ==================== FONCTIONS UTILITAIRES ====================

@st.cache_resource
def load_embeddings():
    """Charge le mod√®le d'embeddings."""
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )


@st.cache_resource
def load_llm():
    """Charge le mod√®le LLM depuis Hugging Face."""
    if not HF_TOKEN:
        st.error("‚ùå Token Hugging Face manquant. V√©rifiez votre fichier .env")
        st.stop()
    
    return HuggingFaceEndpoint(
        repo_id=LLM_MODEL,
        huggingfacehub_api_token=HF_TOKEN,
        temperature=0.3,
        max_new_tokens=512,
        top_k=40,
        top_p=0.95
    )


@st.cache_resource
def load_or_create_vector_store(_embeddings):
    """
    Charge la base vectorielle FAISS existante ou la cr√©e depuis le PDF.
    Le pr√©fixe _ dans _embeddings √©vite le hashing par Streamlit.
    """
    # V√©rification de l'existence du PDF
    if not os.path.exists(PDF_PATH):
        st.error(f"‚ùå Fichier PDF introuvable : {PDF_PATH}")
        st.info("üìù Placez votre m√©moire dans le dossier `data/` avec le nom `memoire.pdf`")
        st.stop()
    
    # Chargement de la base vectorielle existante
    if os.path.exists(VECTOR_STORE_PATH):
        st.info("üìÇ Chargement de la base vectorielle existante...")
        return FAISS.load_local(
            VECTOR_STORE_PATH, 
            _embeddings, 
            allow_dangerous_deserialization=True
        )
    
    # Cr√©ation de la base vectorielle depuis le PDF
    st.info("üîÑ Premi√®re ex√©cution : cr√©ation de la base vectorielle...")
    
    # Chargement du PDF
    loader = PyPDFLoader(PDF_PATH)
    documents = loader.load()
    
    # D√©coupage en chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    
    st.info(f"üìÑ Document d√©coup√© en {len(chunks)} segments")
    
    # Cr√©ation des embeddings et de la base FAISS
    vector_store = FAISS.from_documents(chunks, _embeddings)
    
    # Sauvegarde locale
    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)
    vector_store.save_local(VECTOR_STORE_PATH)
    
    st.success("‚úÖ Base vectorielle cr√©√©e et sauvegard√©e !")
    
    return vector_store


def create_qa_chain(llm, vector_store):
    """Cr√©e la cha√Æne de question-r√©ponse avec RetrievalQA."""
    
    # Template de prompt personnalis√©
    prompt_template = """Tu es un assistant acad√©mique expert sp√©cialis√© dans l'analyse du churn client en e-commerce.
Tu r√©ponds √† des questions sur un m√©moire de recherche en Data Science.

Contexte extrait du m√©moire :
{context}

Question : {question}

Instructions :
- R√©ponds de mani√®re pr√©cise et structur√©e
- Cite les m√©thodes, mod√®les ou r√©sultats mentionn√©s dans le contexte
- Si l'information n'est pas dans le contexte, indique-le clairement
- Utilise un ton acad√©mique mais accessible

R√©ponse :"""

    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    
    # Cr√©ation de la cha√Æne RetrievalQA
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        ),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    
    return qa_chain


# ==================== INTERFACE STREAMLIT ====================

def main():
    """Fonction principale de l'application."""
    
    # En-t√™te
    st.title("üéì Assistant IA - M√©moire de Recherche")
    st.markdown("""
    ### üìö Approches intelligentes et math√©matiques pour analyser le churn client dans le e-commerce
    
    Posez vos questions sur :
    - Les mod√®les de survie (CoxPH, Weibull AFT, DeepSurv, DeepHit)
    - Les m√©thodes d'analyse du churn
    - Les r√©sultats et conclusions du m√©moire
    - Les techniques de feature engineering
    """)
    
    st.divider()
    
    # Chargement des ressources
    with st.spinner("‚öôÔ∏è Initialisation du syst√®me..."):
        embeddings = load_embeddings()
        llm = load_llm()
        vector_store = load_or_create_vector_store(embeddings)
        qa_chain = create_qa_chain(llm, vector_store)
    
    st.success("‚úÖ Syst√®me pr√™t !")
    
    # Zone de saisie de la question
    st.subheader("üí¨ Posez votre question")
    
    question = st.text_area(
        "Question :",
        placeholder="Exemple : Quels sont les principaux mod√®les de survie utilis√©s dans ce m√©moire ?",
        height=100
    )
    
    # Bouton de soumission
    if st.button("üîç Rechercher la r√©ponse", type="primary"):
        if question.strip():
            with st.spinner("ü§î Analyse en cours..."):
                try:
                    # Ex√©cution de la requ√™te
                    result = qa_chain.invoke({"query": question})
                    
                    # Affichage de la r√©ponse
                    st.subheader("üí° R√©ponse :")
                    st.markdown(result["result"])
                    
                    # Affichage des sources (optionnel)
                    with st.expander("üìÑ Sources extraites du m√©moire"):
                        for i, doc in enumerate(result["source_documents"], 1):
                            st.markdown(f"**Extrait {i} (page {doc.metadata.get('page', 'N/A')}) :**")
                            st.text(doc.page_content[:500] + "...")
                            st.divider()
                
                except Exception as e:
                    st.error(f"‚ùå Erreur lors de la recherche : {str(e)}")
        else:
            st.warning("‚ö†Ô∏è Veuillez saisir une question.")
    
    # Footer
    st.divider()
    st.markdown("""
    <div style='text-align: center; color: gray; font-size: 0.9em;'>
        <p>üöÄ Projet d√©velopp√© avec LangChain, FAISS et Streamlit</p>
        <p>üìñ Master Mod√©lisation Math√©matique et Science de Donn√©es</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
